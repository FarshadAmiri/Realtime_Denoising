{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378ce935",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cc841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "D:\\Git_repos\\ClearCast\\Nemo-diarization\\diarization_processor.py:13: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition, EncoderClassifier\n",
      "D:\\Git_repos\\ClearCast\\Nemo-diarization\\diarization_processor.py:13: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition, EncoderClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(r\"D:\\Git_repos\\ClearCast\\Nemo-diarization\")\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import our modules\n",
    "from voice_enrollment import VoiceEnrollment, create_voice_database\n",
    "from diarization_processor import DiarizationProcessor\n",
    "from transcription import TranscriptionProcessor\n",
    "from main import process_meeting_audio, quick_diarize, diarize_and_transcribe\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1936de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HuggingFace token loaded\n",
      "✓ huggingface_hub version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "# Set up HuggingFace token for model downloads\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HF_TOKEN_PATH = Path(r\"D:\\Git_repos\\ClearCast\\hf_token.txt\")\n",
    "\n",
    "if HF_TOKEN_PATH.exists():\n",
    "    with open(HF_TOKEN_PATH, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "        os.environ['HF_TOKEN'] = hf_token\n",
    "        os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\n",
    "        os.environ['HUGGINGFACE_HUB_TOKEN'] = hf_token\n",
    "        os.environ['HF_HOME'] = str(Path.home() / '.cache' / 'huggingface')\n",
    "    print(\"✓ HuggingFace token loaded\")\n",
    "else:\n",
    "    print(\"⚠ HuggingFace token not found at:\", HF_TOKEN_PATH)\n",
    "    print(\"  Models will be downloaded without authentication\")\n",
    "\n",
    "# Fix huggingface_hub version compatibility\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    print(f\"✓ huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "    \n",
    "    # Upgrade if needed\n",
    "    if hasattr(huggingface_hub, 'hf_hub_download'):\n",
    "        import inspect\n",
    "        sig = inspect.signature(huggingface_hub.hf_hub_download)\n",
    "        if 'use_auth_token' in sig.parameters and 'token' not in sig.parameters:\n",
    "            print(\"⚠ Old huggingface_hub version detected. Upgrading...\")\n",
    "            import subprocess\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"huggingface-hub\"])\n",
    "            print(\"✓ huggingface_hub upgraded. Please restart kernel.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f5091",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your file paths here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93cea3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration set\n"
     ]
    }
   ],
   "source": [
    "# ==== CONFIGURE THESE PATHS ====\n",
    "\n",
    "# Meeting audio file to process\n",
    "MEETING_AUDIO = r\"D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\"\n",
    "\n",
    "# Speaker voice samples for enrollment\n",
    "SPEAKER_SAMPLES = {\n",
    "    \"sp3000\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0024.flac\", \n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0040.flac\"],\n",
    "    \"sp777\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0028.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0025.flac\"],\n",
    "    \"sp422\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0021.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0016.flac\"],\n",
    "    \"sp1993\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0005.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0003.flac\"],\n",
    "}\n",
    "\n",
    "# Where to save speaker database\n",
    "SPEAKER_DATABASE = r\"D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\db\\speakers_db.json\"\n",
    "\n",
    "# Optional: Path to custom Whisper model\n",
    "WHISPER_MODEL_PATH = r\"C:\\Users\\User_1\\.cache\\huggingface\\hub\\models--openai--whisper-medium\"  # or \"path/to/whisper_medium.pt\"\n",
    "\n",
    "# Language (en, fa, ar, etc.) or None for auto-detect\n",
    "LANGUAGE = \"en\"  # Change to \"fa\" for Persian\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = r\"D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\"\n",
    "\n",
    "print(\"✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abaae3",
   "metadata": {},
   "source": [
    "## Step 1: Create Speaker Database\n",
    "\n",
    "Enroll speakers by providing reference audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ec21a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating speaker database...\n",
      "Loaded the voice encoder model on cuda in 0.02 seconds.\n",
      "✓ Loaded 4 speakers from database\n",
      "✓ Enrolled speaker: sp3000 (from 2 samples)\n",
      "✓ Enrolled speaker: sp777 (from 2 samples)\n",
      "✓ Enrolled speaker: sp422 (from 2 samples)\n",
      "✓ Enrolled speaker: sp1993 (from 2 samples)\n",
      "✓ Database saved to: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\db\\speakers_db.json\n",
      "\n",
      "✓ Database created with 4 speakers\n",
      "Enrolled speakers: ['sp3000', 'sp777', 'sp422', 'sp1993']\n",
      "✓ Enrolled speaker: sp422 (from 2 samples)\n",
      "✓ Enrolled speaker: sp1993 (from 2 samples)\n",
      "✓ Database saved to: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\db\\speakers_db.json\n",
      "\n",
      "✓ Database created with 4 speakers\n",
      "Enrolled speakers: ['sp3000', 'sp777', 'sp422', 'sp1993']\n"
     ]
    }
   ],
   "source": [
    "# Create voice database from samples\n",
    "print(\"Creating speaker database...\")\n",
    "\n",
    "enrollment = create_voice_database(\n",
    "    database_path=SPEAKER_DATABASE,\n",
    "    speaker_samples=SPEAKER_SAMPLES\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Database created with {len(enrollment.get_all_speakers())} speakers\")\n",
    "print(f\"Enrolled speakers: {enrollment.get_all_speakers()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1644f8",
   "metadata": {},
   "source": [
    "## Step 2: Quick Diarization (No Transcription)\n",
    "\n",
    "Perform speaker diarization to detect \"who spoke when\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f9769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing diarization...\n",
      "\n",
      "============================================================\n",
      "SPEAKER DIARIZATION PIPELINE\n",
      "============================================================\n",
      "Audio file: D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\n",
      "Voice database: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\db\\speakers_db.json\n",
      "Language: auto-detect\n",
      "Transcription: disabled\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading speaker database...\n",
      "Loaded the voice encoder model on cuda in 0.01 seconds.\n",
      "✓ Loaded 4 speakers from database\n",
      "✓ Loaded 4 enrolled speakers: ['sp3000', 'sp777', 'sp422', 'sp1993']\n",
      "\n",
      "[2/4] Performing speaker diarization...\n",
      "Using device: cuda\n",
      "Loading SpeechBrain models...\n",
      "Warning: Could not load SpeechBrain model: hf_hub_download() got an unexpected keyword argument 'use_auth_token'\n",
      "Continuing with Resemblyzer only...\n",
      "Loaded the voice encoder model on cuda in 0.01 seconds.\n",
      "✓ Models loaded successfully\n",
      "Processing: D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\n",
      "Extracting speaker embeddings...\n",
      "Extracted 113 embeddings\n",
      "Clustering speakers...\n",
      "✓ Found 8 speakers\n",
      "✓ Generated 12 segments\n",
      "✓ Results saved to: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_raw.json\n",
      "\n",
      "[3/4] Identifying speakers...\n",
      "Extracted 113 embeddings\n",
      "Clustering speakers...\n",
      "✓ Found 8 speakers\n",
      "✓ Generated 12 segments\n",
      "✓ Results saved to: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_raw.json\n",
      "\n",
      "[3/4] Identifying speakers...\n",
      "✓ Identified 10/12 segments\n",
      "\n",
      "[4/4] Skipping transcription (disabled)\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Detected speakers: 6\n",
      "Identified speakers: ['sp777', 'sp422', 'sp1993', 'sp3000']\n",
      "Total segments: 12\n",
      "Output directory: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "Number of speakers detected: 6\n",
      "Identified speakers: ['sp777', 'sp422', 'sp1993', 'sp3000']\n",
      "Total segments: 12\n",
      "\n",
      "Output files:\n",
      "  diarization_raw: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_raw.json\n",
      "  diarization_identified: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_identified.json\n",
      "✓ Identified 10/12 segments\n",
      "\n",
      "[4/4] Skipping transcription (disabled)\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Detected speakers: 6\n",
      "Identified speakers: ['sp777', 'sp422', 'sp1993', 'sp3000']\n",
      "Total segments: 12\n",
      "Output directory: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "Number of speakers detected: 6\n",
      "Identified speakers: ['sp777', 'sp422', 'sp1993', 'sp3000']\n",
      "Total segments: 12\n",
      "\n",
      "Output files:\n",
      "  diarization_raw: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_raw.json\n",
      "  diarization_identified: D:\\Git_repos\\ClearCast\\Nemo-diarization\\outputs\\files\\diarization_identified.json\n"
     ]
    }
   ],
   "source": [
    "# Quick diarization without transcription\n",
    "print(\"Performing diarization...\\n\")\n",
    "\n",
    "result = quick_diarize(\n",
    "    audio_path=MEETING_AUDIO,\n",
    "    database_path=SPEAKER_DATABASE,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Number of speakers detected: {result['num_speakers']}\")\n",
    "print(f\"Identified speakers: {result['identified_speakers']}\")\n",
    "print(f\"Total segments: {len(result['segments'])}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "for key, path in result['output_files'].items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343ef35",
   "metadata": {},
   "source": [
    "## Step 3: View Diarization Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ca1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization Segments:\n",
      "============================================================\n",
      "\n",
      "1. [  0.75s -  18.00s] ✓ sp3000\n",
      "2. [ 18.00s -  18.75s] ? Speaker_5\n",
      "3. [ 18.75s -  30.00s] ✓ sp422\n",
      "4. [ 30.00s -  30.75s] ? Speaker_7\n",
      "5. [ 30.75s -  36.00s] ✓ sp422\n",
      "6. [ 36.00s -  37.50s] ✓ sp777\n",
      "7. [ 37.50s -  40.50s] ✓ sp777\n",
      "8. [ 40.50s -  52.50s] ✓ sp3000\n",
      "9. [ 52.50s -  54.00s] ✓ sp3000\n",
      "10. [ 54.00s -  63.00s] ✓ sp3000\n",
      "11. [ 63.00s -  74.25s] ✓ sp777\n",
      "12. [ 74.25s -  84.75s] ✓ sp1993\n"
     ]
    }
   ],
   "source": [
    "# Display first 10 segments\n",
    "print(\"Diarization Segments:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, seg in enumerate(result['segments'], 1):\n",
    "    start = seg['start']\n",
    "    end = seg['end']\n",
    "    speaker = seg['speaker']\n",
    "    identified = \"✓\" if seg.get('identified', False) else \"?\"\n",
    "    \n",
    "    print(f\"{i}. [{start:6.2f}s - {end:6.2f}s] {identified} {speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc4338",
   "metadata": {},
   "source": [
    "## Step 4: Full Pipeline with Transcription\n",
    "\n",
    "Run complete pipeline with speaker diarization + transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ab4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: Diarization + Transcription\n",
    "print(\"Running full pipeline with transcription...\\n\")\n",
    "\n",
    "result_full = process_meeting_audio(\n",
    "    meeting_audio_path=MEETING_AUDIO,\n",
    "    voice_embeddings_database_path=SPEAKER_DATABASE,\n",
    "    expected_language=LANGUAGE,\n",
    "    output_transcriptions=True,\n",
    "    transcriptor_model_path=WHISPER_MODEL_PATH,\n",
    "    output_dir=OUTPUT_DIR + \"_with_transcript\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPLETE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Speakers detected: {result_full['num_speakers']}\")\n",
    "print(f\"Identified: {result_full['identified_speakers']}\")\n",
    "print(f\"Segments: {len(result_full['segments'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022672",
   "metadata": {},
   "source": [
    "## Step 5: View Transcript with Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcript with speaker labels\n",
    "if 'transcription' in result_full:\n",
    "    print(\"TRANSCRIPT WITH SPEAKER LABELS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Show first 10 segments with text\n",
    "    for i, seg in enumerate(result_full['segments'][:10], 1):\n",
    "        if 'text' in seg:\n",
    "            print(f\"[{seg['start']:6.2f}s - {seg['end']:6.2f}s]\")\n",
    "            print(f\"{seg['speaker']}: {seg['text']}\")\n",
    "            print()\n",
    "    \n",
    "    if len(result_full['segments']) > 10:\n",
    "        print(f\"... and {len(result_full['segments']) - 10} more segments\\n\")\n",
    "    \n",
    "    # Full transcript\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FULL TRANSCRIPT\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(result_full['transcription'])\n",
    "else:\n",
    "    print(\"No transcription available (transcription was disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da872cb2",
   "metadata": {},
   "source": [
    "## Step 6: Alternative - Using Direct Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2373575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method: Direct function call with all parameters\n",
    "result_alt = diarize_and_transcribe(\n",
    "    audio_path=MEETING_AUDIO,\n",
    "    database_path=SPEAKER_DATABASE,\n",
    "    language=LANGUAGE,\n",
    "    whisper_model=WHISPER_MODEL_PATH,\n",
    "    output_dir=\"diarization_alternative\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Processing complete\")\n",
    "print(f\"Results saved to: {result_alt['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be0bdd",
   "metadata": {},
   "source": [
    "## Step 7: Save Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4faca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create summary\n",
    "summary = {\n",
    "    \"audio_file\": MEETING_AUDIO,\n",
    "    \"num_speakers\": result_full['num_speakers'],\n",
    "    \"identified_speakers\": result_full['identified_speakers'],\n",
    "    \"enrolled_speakers\": result_full['enrolled_speakers'],\n",
    "    \"total_segments\": len(result_full['segments']),\n",
    "    \"output_files\": result_full['output_files']\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = Path(result_full['output_dir']) / \"summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary saved to: {summary_path}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76148513",
   "metadata": {},
   "source": [
    "## Advanced: Manual Testing of Components\n",
    "\n",
    "Test individual components separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Voice Enrollment\n",
    "print(\"Testing Voice Enrollment...\")\n",
    "test_enrollment = VoiceEnrollment(\"test_database.json\")\n",
    "\n",
    "# Enroll a test speaker (replace with actual audio path)\n",
    "# test_enrollment.enroll_speaker(\"TestSpeaker\", \"test_audio.wav\")\n",
    "# test_enrollment.save_database()\n",
    "\n",
    "print(\"✓ Voice Enrollment working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Diarization Processor\n",
    "print(\"Testing Diarization Processor...\")\n",
    "test_diarizer = DiarizationProcessor()\n",
    "\n",
    "# Perform diarization (replace with actual audio path)\n",
    "# test_segments = test_diarizer.perform_diarization(\"test_meeting.wav\")\n",
    "# print(f\"Found {len(test_segments)} segments\")\n",
    "\n",
    "print(\"✓ Diarization Processor working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35797ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Transcription Processor\n",
    "print(\"Testing Transcription Processor...\")\n",
    "test_transcriptor = TranscriptionProcessor(model_name=\"base\")\n",
    "\n",
    "# Transcribe audio (replace with actual audio path)\n",
    "# test_result = test_transcriptor.transcribe_audio(\"test_audio.wav\", language=\"en\")\n",
    "# print(f\"Transcribed {len(test_result['segments'])} segments\")\n",
    "\n",
    "print(\"✓ Transcription Processor working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0519e",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "1. **GPU Memory Error**: Use smaller Whisper model or switch to CPU\n",
    "2. **Poor Identification**: Enroll speakers with longer/multiple samples\n",
    "3. **Wrong Speaker Count**: Specify `num_speakers` parameter\n",
    "4. **Language Issues**: Explicitly set `expected_language` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "import torch\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e3a3c",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Your diarization system is now ready to use. Check the output directory for all generated files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
